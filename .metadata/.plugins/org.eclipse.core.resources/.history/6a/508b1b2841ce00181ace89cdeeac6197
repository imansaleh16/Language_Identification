package lang_id;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.HashMap;

import org.nustaq.serialization.FSTObjectInput;
import org.nustaq.serialization.FSTObjectOutput;

public class NaiveBayes {
	
	private HashMap<String, HashMap<String, Integer>> hmshmsiLangNGramCounts = new HashMap<String, HashMap<String, Integer>>();
	//DB db;
    //private ConcurrentMap<String, HashMap<String, Integer>> hmshmsiLangNGramCounts;
	private HashMap<String, Integer> hmsiLangTokensCount = new HashMap<String, Integer>();
	private HashMap<String, Double> hmsdLangProb = new HashMap<String, Double>();
	private HashMap<String, String> hmssLangNames = new HashMap<String, String>();
	private HashMap<String, HashMap<String, Integer>> hmshmsiConfusionMatrix = new HashMap<String, HashMap<String, Integer>>();
	
	private int iV = 0;
	private static int IGRAM = 10;
	
	private final String S_LANG_NGRAM_COUNTS = IGRAM + "gram_count_per_lang";
	private final String S_LANG_TOTAL_TOKENS_COUNTS = IGRAM + "gram_tokens_count_per_lang";
	private final String S_VOC_SIZE = IGRAM + "gram_voc_size";
	private final String S_LANG_PROP = IGRAM + "gram_lang_props";
	private final String S_LANG_NAMES = "lang_names";
	
	private final String S_MODEL_DATA_FOLDER = "model_data/";
	private static final String S_DATA_FOLDER = "data/";
	
	public static void main(String[] args) throws IOException{
		NaiveBayes nb = new NaiveBayes();
		
		nb.loadLabels(S_DATA_FOLDER + "labels.csv");
		nb.createTrainCounts(S_DATA_FOLDER + "docsMR.txt", S_DATA_FOLDER + "docsMR.labels", IGRAM);
		nb.test(S_DATA_FOLDER + "docsME.txt", S_DATA_FOLDER + "docsME.labels", IGRAM);
	}
	
	private void loadLabels(String sFile) throws IOException
	{
		BufferedReader brLabels = new BufferedReader(new FileReader(sFile));
		
		String sLine = "";
		String[] saTokens;
		
		sLine=brLabels.readLine();
		
		while((sLine=brLabels.readLine())!=null)
		{
			saTokens = sLine.split(";");
			System.out.println(saTokens[0] + " = " + saTokens[1]);
			//hmssLangNames.put(saTokens[0], saTokens[1]);
			hmssLangNames.put(saTokens[2], saTokens[1]);
		}
		
		Serializer.serializeDictionary(hmssLangNames, S_MODEL_DATA_FOLDER + S_LANG_NAMES);
		brLabels.close();
	}
	
	public void createTrainCounts(String sTrainFile, String sLabelsFile, 
			int iGramSize) throws IOException
	{
		BufferedReader brTrain = new BufferedReader(new FileReader(sTrainFile));
		BufferedReader brTrainLabels = new BufferedReader(new FileReader(sLabelsFile));
		
		String sLine = "";
		String sGram = "";
		String sInstLang = "";

		int iTotalDocs = 0;
		
		HashMap<String, Integer> hmsiNGramCounts;
		
		//new File(sModelDataFolder + "ngrams.db").deleteOnExit();
		//openLangNGramCountDB();
		
		/*
		 * Traverse training file line by line, each line contains
		 * one document.
		 */
		while((sLine=brTrain.readLine())!=null)
		{
			iTotalDocs ++;
			
			// Read the language of the document.
			sInstLang = brTrainLabels.readLine();
			//System.out.println(sInstLang);
			
			/* 
			 * Store number of documents belonging to a language to calculate
			 * p(lang) = count of lang docs / total docs
			 */
			if(hmsdLangProb.containsKey(sInstLang))
				hmsdLangProb.put(sInstLang, hmsdLangProb.get(sInstLang)+1);
			else
				hmsdLangProb.put(sInstLang, 1.0);
			
			// Create ngrams and store them for each language.
			for(int i=0; i<sLine.length()-iGramSize+1; i++)
			{
				sGram = sLine.substring(i, i+iGramSize);
				System.out.println(sGram);
				
				// Language exists in dictionary of languages tokens. 
				if(hmshmsiLangNGramCounts.containsKey(sInstLang))
				{
					// Get ngram counts dictionary for the language
					hmsiNGramCounts = hmshmsiLangNGramCounts.get(sInstLang);
					/* 
					 * Store total count of ngrams for a language, one more ngram
					 * found.
					 */
					hmsiLangTokensCount.put(sInstLang, hmsiLangTokensCount.get(sInstLang)+1);
				}
				else
				{
					hmsiNGramCounts = new HashMap<String, Integer>();
					hmsiLangTokensCount.put(sInstLang, 1);
				}
				
				// Check if the ngram was seen before for this language.
				if(hmsiNGramCounts.containsKey(sGram))
				{
					hmsiNGramCounts.put(sGram, hmsiNGramCounts.get(sGram)+1);
				}
				else
				{
					hmsiNGramCounts.put(sGram, 1);
				}
				
				hmshmsiLangNGramCounts.put(sInstLang, hmsiNGramCounts);
			}
		}
		
		/*
		 * Count the vocabulary, used for Laplace smoothing.
		 * Total number of unique ngrams in all languages.
		 */
		int iTotalTest;
		for(String sLanguage:hmshmsiLangNGramCounts.keySet())
		{
			iTotalTest = 0;
			hmsiNGramCounts = hmshmsiLangNGramCounts.get(sLanguage);
			// Count vocabulary of all languages, unique ngrams only
			iV += hmsiNGramCounts.size();
			for(String g :hmsiNGramCounts.keySet())
			{
				iTotalTest += hmsiNGramCounts.get(g);
			}
			System.out.println(sLanguage + " ngrams = " +  iTotalTest 
					+ ", " + hmsiLangTokensCount.get(sLanguage));
		}
		
		/*
		 * Calculate p(lang)
		 */
		double dProb;
		for(String sLanguage:hmsdLangProb.keySet())
		{
			dProb = (double)hmsdLangProb.get(sLanguage)/(double)iTotalDocs;
			hmsdLangProb.put(sLanguage, dProb);
			System.out.println(sLanguage + " probability = " + dProb);
		}
		
		System.out.println("Total Vocabulary = " + iV);
		
		/*
		 * Serialize dictionaries and vocabulary size to use it in testing.
		 */
		Serializer.serializeDictionary(hmshmsiLangNGramCounts, S_MODEL_DATA_FOLDER + S_LANG_NGRAM_COUNTS);
		//closeLangNGramCountDB();
		Serializer.serializeDictionary(hmsiLangTokensCount, S_MODEL_DATA_FOLDER + S_LANG_TOTAL_TOKENS_COUNTS);
		Serializer.serializeDictionary(hmsdLangProb, S_MODEL_DATA_FOLDER + S_LANG_PROP);
		
		storeVocabSize();
		
		brTrain.close();
		brTrainLabels.close();
	}
	/*
	private void closeLangNGramCountDB() {
		db.close();		
	}
	*/
	private void storeVocabSize() throws IOException{
		BufferedWriter bwVocab = new BufferedWriter(new FileWriter(new File(S_MODEL_DATA_FOLDER + S_VOC_SIZE)));
		bwVocab.write(String.valueOf(iV));
		bwVocab.close();		
	}
	
	private void readVocabSize() throws IOException{
		BufferedReader bwVocab = new BufferedReader(new FileReader(new File(S_MODEL_DATA_FOLDER + S_VOC_SIZE)));
		iV = Integer.parseInt(bwVocab.readLine());
		bwVocab.close();
	}
	/*
	private void openLangNGramCountDB()
	{
		db = DBMaker
	            .fileDB(sModelDataFolder + "ngrams.db")
	            .transactionEnable()
	            .closeOnJvmShutdown()
	            .fileChannelEnable()
	            .fileMmapEnable()
	            .make();
	    hmshmsiLangNGramCounts = db.hashMap("map", Serializer.STRING, Serializer.ELSA).createOrOpen();
	}
	*/
	
	public void test(String sTestFile, String sTestFileLabels, int iGramSize) throws IOException
	{
		hmsiLangTokensCount = Serializer.deserialize_hm_s_i(S_MODEL_DATA_FOLDER + S_LANG_TOTAL_TOKENS_COUNTS);
		hmshmsiLangNGramCounts = Serializer.deserialize_hm_s_hmsi(S_MODEL_DATA_FOLDER + S_LANG_NGRAM_COUNTS);
		//openLangNGramCountDB();
		hmsdLangProb = Serializer.deserialize_hm_s_d(S_MODEL_DATA_FOLDER + S_LANG_PROP);
		hmssLangNames = Serializer.deserialize_hm_s_s(S_MODEL_DATA_FOLDER + S_LANG_NAMES);
		
		HashMap<String, Integer> hmsiSubConfusionMatrix;
		
		readVocabSize();
		System.out.println("Models loaded ... ");
		
		BufferedReader brTest = new BufferedReader(new FileReader(sTestFile));
		BufferedReader brTestLabels = new BufferedReader(new FileReader(sTestFileLabels));
		
		BufferedWriter bw = new BufferedWriter(new FileWriter("answer"));
		
		String sLine = "";
		
		String sInstLang = "";
		String sAnswer = "";
		
		int iTotalCorrect = 0;
		int iTotalDocs = 0;
		
		// Stores probability of document belonging to each language
		HashMap<String, Double> hmsdLangProbTest = new HashMap<String, Double>();
		
		
		while((sLine=brTest.readLine())!=null)
		{
			iTotalDocs ++;
			
			// Read the language of the document.
			sInstLang = brTestLabels.readLine();
			
			hmsdLangProbTest = calculateLangProbs(sLine, iGramSize);
			
			sAnswer = getMaxProb(hmsdLangProbTest);
			
			bw.write(sAnswer + "\n");
			//System.out.println(sAnswer + ", Gold lang = " + sInstLang);
			
			if(hmshmsiConfusionMatrix.containsKey(sInstLang))
			{
				hmsiSubConfusionMatrix = hmshmsiConfusionMatrix.get(sInstLang);
			}
			else{
				hmsiSubConfusionMatrix = new HashMap<String,Integer>();
			}
			
			if(hmsiSubConfusionMatrix.containsKey(sAnswer))
			{
				hmsiSubConfusionMatrix.put(sAnswer, hmsiSubConfusionMatrix.get(sAnswer)+1);
			}
			else
			{
				hmsiSubConfusionMatrix.put(sAnswer, 1);
			}
			
			hmshmsiConfusionMatrix.put(sInstLang, hmsiSubConfusionMatrix);
			
			if(sAnswer.compareTo(sInstLang)==0)
			{
				iTotalCorrect ++;
			}
		}
		System.out.println("Correct = " + iTotalCorrect 
				+ ", Total = " + iTotalDocs);
		
		calculatePrecisionRecallF1();
		brTest.close();
		brTestLabels.close();
		bw.close();
	}
	
	private String getMaxProb(HashMap<String, Double> hmsdLangProbTest) {
		double dMaxProb = -1000000.0;
		String sAnswer = "";
		
		// Output most probable language
		for(String sLanguage:hmsdLangProbTest.keySet())
		{
			if(hmsdLangProbTest.get(sLanguage) > dMaxProb)
			{
				dMaxProb = hmsdLangProbTest.get(sLanguage);
				sAnswer = sLanguage;
			}
		}
		return sAnswer;
	}
	
	private void calculatePrecisionRecallF1()
	{
		int iTP;
		int iFP;
		int iFN;
		
		int iTPAvg = 0;
		int iFPAvg = 0;
		int iFNAvg = 0;
		
		double dPrecision;
		double dRecall;
		double dF1;
		
		double dF1Macro = 0;
		double dPMacro = 0;
		double dRMacro = 0;
		
		double dF1Micro = 0;
		double dPMicro = 0;
		double dRMicro = 0;
		
		HashMap<String, Integer> hm;
		/* 
		 * precision = tp / tp + fp
		 * recall = tp / tp + fn
		 */
		for(String sLanguage:hmshmsiConfusionMatrix.keySet())
		{
			hm = hmshmsiConfusionMatrix.get(sLanguage);
			
			if(hm.containsKey(sLanguage))
				iTP = hm.get(sLanguage);
			else
				iTP = 0;
			
			iFP = 0;
			iFN = 0;
			
			for(String sLanguage_2:hm.keySet())
			{
				if(sLanguage.compareTo(sLanguage_2)!=0)
					iFN += hm.get(sLanguage_2);
			}
			
			for(String sLanguage_2:hmshmsiConfusionMatrix.keySet())
			{
				if(sLanguage_2.compareTo(sLanguage)!=0)
				{
					hm = hmshmsiConfusionMatrix.get(sLanguage_2);
					if(hm.containsKey(sLanguage))
						iFP += hm.get(sLanguage);
				}
			}
			
			if(iTP==0 && iFP==0)
				dPrecision = 0;
			else
				dPrecision = (double)iTP/((double)iTP + (double)iFP);
			
			if(iTP==0 && iFN==0)
				dRecall = 0;
			else
				dRecall = (double)iTP/((double)iTP + (double)iFN);
			
			if(dPrecision==0 && dRecall==0)
				dF1 = 0;
			else
				dF1 = (2 * dPrecision * dRecall) / (dPrecision + dRecall);
			
			System.out.println(sLanguage + ", tp = " + iTP + ", fp = " + iFP
					+ ", fn = " + iFN + ", precision = " + dPrecision 
					+ ", recall = " + dRecall + ", F1 = " + dF1);
			
			dF1Macro += dF1;
			dPMacro += dPrecision;
			dRMacro += dRecall;
			
			iTPAvg += iTP;
			iFPAvg += iFP;
			iFNAvg += iFN;
		}
		System.out.println(dF1Macro);
		System.out.println(hmsdLangProb.size());
		dF1Macro /= hmsdLangProb.size();
		
		dPMicro = (double)iTPAvg / (double)(iTPAvg + iFPAvg);
		dRMicro = (double)iTPAvg / (double)(iTPAvg + iFNAvg);
		dF1Micro = (2 * dRMicro * dPMicro)/(dPMicro + dRMicro);
		
		System.out.println("Macro F1 = " + dF1Macro + ", micro F1 = " + dF1Micro);
	}

	public void getDocLanguage(String sDocPath) throws IOException
	{
		//openLangNGramCountDB();
		if(!new File(sDocPath).exists())
		{
			System.out.println("This file does not exist ...");
			return;
		}
		
		String sAnswer = "";
		String sLine = "";
		String sDoc = "";
		
		HashMap<String, Double> hmsdLangProbTest = new HashMap<String, Double>();
		
		System.out.println("Loading models ... ");
		
		hmsiLangTokensCount = Serializer.deserialize_hm_s_i(S_MODEL_DATA_FOLDER + S_LANG_TOTAL_TOKENS_COUNTS);
		hmshmsiLangNGramCounts = Serializer.deserialize_hm_s_hmsi(S_MODEL_DATA_FOLDER + S_LANG_NGRAM_COUNTS);
		hmsdLangProb = Serializer.deserializeLangProps(S_MODEL_DATA_FOLDER + S_LANG_PROP);
		hmssLangNames = Serializer.deserializeLangNames(S_MODEL_DATA_FOLDER + S_LANG_NAMES);
		
		readVocabSize();
		
		System.out.println("Models loaded ... ");
		
		BufferedReader br = new BufferedReader(new FileReader(sDocPath));
		while((sLine=br.readLine())!=null)
		{
			sDoc += sLine + " ";
		}
		br.close();
		sDoc = sDoc.trim();
		
		hmsdLangProbTest = calculateLangProbs(sDoc, IGRAM);
		sAnswer = getMaxProb(hmsdLangProbTest);
		System.out.println(hmssLangNames.get(sAnswer));
	}
	
	private HashMap<String, Double> calculateLangProbs (String sDoc, int iGramSize){
		String sGram = "";
		int iNGramCount;
		double dNGramProb;
		
		HashMap<String, Integer> hmsiNGramCounts;
		HashMap<String, Double> hmsdLangProbTest = new HashMap<String, Double>();
		
		
		// Create ngrams and store them for each language.
		for(int i=0; i<sDoc.length()-iGramSize+1; i++)
		{
			sGram = sDoc.substring(i, i+iGramSize);
			//System.out.println(sGram);
			/*
			 * For each language, lookup the ngram, get its frequency,
			 * divide by total number of tokens in the language, get the
			 * logarithm of quotient, and sum.
			 */
			for(String sLanguage:hmshmsiLangNGramCounts.keySet())
			{
				/* 
				 * Initialize probability of document belonging to
				 * a language to log(prob(lang)).
				 */
				hmsdLangProbTest.put(sLanguage, Math.log(hmsdLangProb.get(sLanguage)));
			}
			
			for(String sLanguage:hmshmsiLangNGramCounts.keySet())
			{
				hmsiNGramCounts = hmshmsiLangNGramCounts.get(sLanguage);
				
				// Get count of ngram for this language.
				if(hmsiNGramCounts.containsKey(sGram))
				{
					iNGramCount = hmsiNGramCounts.get(sGram);
				}
				else{
					iNGramCount = 0;
				}
				
				/* 
				 * Add 1 for Laplace smoothing. 
				 * Divide by total number of tokens for the language
				 * + vocabulary size. The latter is for Laplace smoothing.
				 */
				dNGramProb = (double)(iNGramCount + 1)/(double)(hmsiLangTokensCount.get(sLanguage) + iV);
				/* 
				 * Calculate probability of doc belonging to language 
				 * log(prob(lang))+ sum log(prob(w_i|lang_j))
				 */
				hmsdLangProbTest.put(sLanguage, hmsdLangProbTest.get(sLanguage) + Math.log(dNGramProb));
			}
		}
		//System.out.println(hmsdLangProbTest);
		return hmsdLangProbTest;
	}
	
	private void addToConfusion(String sAnswer, String sGold)
	{
		
	}
}
